{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1591169877020,
     "user": {
      "displayName": "Sujin Lee",
      "photoUrl": "",
      "userId": "12954063167699440993"
     },
     "user_tz": -540
    },
    "id": "d4hR8cDcU8PT",
    "outputId": "29105bc0-3605-4f5f-c504-4c5c1b9c627f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et4EWaZ0JOBN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/voicefilter-master')\n",
    "\n",
    "#from utils.audio import Audio\n",
    "#from utils.hparams import HParam\n",
    "#from model.model import VoiceFilter\n",
    "#from model.embedder import SpeechEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HC8XwLIozspP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = nn.Linear(hp.embedder.lstm_hidden, hp.embedder.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class SpeechEmbedder(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(SpeechEmbedder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hp.embedder.num_mels,\n",
    "                            hp.embedder.lstm_hidden,\n",
    "                            num_layers=hp.embedder.lstm_layers,\n",
    "                            batch_first=True)\n",
    "        self.proj = LinearNorm(hp)\n",
    "        self.hp = hp\n",
    "\n",
    "    def forward(self, mel):\n",
    "        # (num_mels, T)\n",
    "        mels = mel.unfold(1, self.hp.embedder.window, self.hp.embedder.stride) # (num_mels, T', window)\n",
    "        mels = mels.permute(1, 2, 0) # (T', window, num_mels)\n",
    "        x, _ = self.lstm(mels) # (T', window, lstm_hidden)\n",
    "        x = x[:, -1, :] # (T', lstm_hidden), use last frame only\n",
    "        x = self.proj(x) # (T', emb_dim)\n",
    "        x = x / torch.norm(x, p=2, dim=1, keepdim=True) # (T', emb_dim)\n",
    "        x = x.sum(0) / x.size(0) # (emb_dim), average pooling over time frames\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VoiceFilter(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(VoiceFilter, self).__init__()\n",
    "        self.hp = hp\n",
    "        assert hp.audio.n_fft // 2 + 1 == hp.audio.num_freq == hp.model.fc2_dim, \\\n",
    "            \"stft-related dimension mismatch\"\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # cnn1\n",
    "            nn.ZeroPad2d((3, 3, 0, 0)),\n",
    "            nn.Conv2d(1, 64, kernel_size=(1, 7), dilation=(1, 1)),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn2\n",
    "            nn.ZeroPad2d((0, 0, 3, 3)),\n",
    "            nn.Conv2d(64, 64, kernel_size=(7, 1), dilation=(1, 1)),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn3\n",
    "            nn.ZeroPad2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(1, 1)),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn4\n",
    "            nn.ZeroPad2d((2, 2, 4, 4)),\n",
    "            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(2, 1)), # (9, 5)\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn5\n",
    "            nn.ZeroPad2d((2, 2, 8, 8)),\n",
    "            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(4, 1)), # (17, 5)\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn6\n",
    "            nn.ZeroPad2d((2, 2, 16, 16)),\n",
    "            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(8, 1)), # (33, 5)\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn7\n",
    "            nn.ZeroPad2d((2, 2, 32, 32)),\n",
    "            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(16, 1)), # (65, 5)\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "\n",
    "            # cnn8\n",
    "            nn.Conv2d(64, 8, kernel_size=(1, 1), dilation=(1, 1)), \n",
    "            nn.BatchNorm2d(8), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            8*hp.audio.num_freq + hp.embedder.emb_dim,\n",
    "            hp.model.lstm_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(2*hp.model.lstm_dim, hp.model.fc1_dim)\n",
    "        self.fc2 = nn.Linear(hp.model.fc1_dim, hp.model.fc2_dim)\n",
    "\n",
    "    def forward(self, x, dvec):\n",
    "        # x: [B, T, num_freq]\n",
    "        x = x.unsqueeze(1)\n",
    "        # x: [B, 1, T, num_freq]\n",
    "        x = self.conv(x)\n",
    "        # x: [B, 8, T, num_freq]\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        # x: [B, T, 8, num_freq]\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        # x: [B, T, 8*num_freq]\n",
    "\n",
    "        # dvec: [B, emb_dim]\n",
    "        dvec = dvec.unsqueeze(1)\n",
    "        dvec = dvec.repeat(1, x.size(1), 1)\n",
    "        # dvec: [B, T, emb_dim]\n",
    "\n",
    "        x = torch.cat((x, dvec), dim=2) # [B, T, 8*num_freq + emb_dim]\n",
    "\n",
    "        x, _ = self.lstm(x) # [B, T, 2*lstm_dim]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x) # x: [B, T, fc1_dim]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x) # x: [B, T, fc2_dim], fc2_dim == num_freq\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_hparam_str(hp_str):\n",
    "    path = os.path.join('config', 'temp-restore.yaml')\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(hp_str)\n",
    "    return HParam(path)\n",
    "\n",
    "\n",
    "def load_hparam(filename):\n",
    "    stream = open(filename, 'r')\n",
    "    docs = yaml.load_all(stream)\n",
    "    hparam_dict = dict()\n",
    "    for doc in docs:\n",
    "        for k, v in doc.items():\n",
    "            hparam_dict[k] = v\n",
    "    return hparam_dict\n",
    "\n",
    "\n",
    "def merge_dict(user, default):\n",
    "    if isinstance(user, dict) and isinstance(default, dict):\n",
    "        for k, v in default.items():\n",
    "            if k not in user:\n",
    "                user[k] = v\n",
    "            else:\n",
    "                user[k] = merge_dict(user[k], v)\n",
    "    return user\n",
    "\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"\n",
    "    a dictionary that supports dot notation \n",
    "    as well as dictionary access notation \n",
    "    usage: d = DotDict() or d = DotDict({'val1':'first'})\n",
    "    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n",
    "    get attributes: d.val2 or d['val2']\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __init__(self, dct=None):\n",
    "        dct = dict() if not dct else dct\n",
    "        for key, value in dct.items():\n",
    "            if hasattr(value, 'keys'):\n",
    "                value = Dotdict(value)\n",
    "            self[key] = value\n",
    "\n",
    "\n",
    "class HParam(Dotdict):\n",
    "\n",
    "    def __init__(self, file):\n",
    "        super(Dotdict, self).__init__()\n",
    "        hp_dict = load_hparam(file)\n",
    "        hp_dotdict = Dotdict(hp_dict)\n",
    "        for k, v in hp_dotdict.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "    __getattr__ = Dotdict.__getitem__\n",
    "    __setattr__ = Dotdict.__setitem__\n",
    "    __delattr__ = Dotdict.__delitem__\n",
    "\n",
    "class Audio():\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "        self.mel_basis = librosa.filters.mel(sr=hp.audio.sample_rate,\n",
    "                                             n_fft=hp.embedder.n_fft,\n",
    "                                             n_mels=hp.embedder.num_mels)\n",
    "\n",
    "    def get_mel(self, y):\n",
    "        y = librosa.core.stft(y=y, n_fft=self.hp.embedder.n_fft,\n",
    "                              hop_length=self.hp.audio.hop_length,\n",
    "                              win_length=self.hp.audio.win_length,\n",
    "                              window='hann')\n",
    "        magnitudes = np.abs(y) ** 2\n",
    "        mel = np.log10(np.dot(self.mel_basis, magnitudes) + 1e-6)\n",
    "        return mel\n",
    "\n",
    "    def wav2spec(self, y):\n",
    "        D = self.stft(y)\n",
    "        S = self.amp_to_db(np.abs(D)) - self.hp.audio.ref_level_db\n",
    "        S, D = self.normalize(S), np.angle(D)\n",
    "        S, D = S.T, D.T # to make [time, freq]\n",
    "        return S, D\n",
    "\n",
    "    def spec2wav(self, spectrogram, phase):\n",
    "        spectrogram, phase = spectrogram.T, phase.T\n",
    "        # used during inference only\n",
    "        # spectrogram: enhanced output\n",
    "        # phase: use noisy input's phase, so no GLA is required\n",
    "        S = self.db_to_amp(self.denormalize(spectrogram) + self.hp.audio.ref_level_db)\n",
    "        return self.istft(S, phase)\n",
    "\n",
    "    def stft(self, y):\n",
    "        return librosa.stft(y=y, n_fft=self.hp.audio.n_fft,\n",
    "                            hop_length=self.hp.audio.hop_length,\n",
    "                            win_length=self.hp.audio.win_length)\n",
    "\n",
    "    def istft(self, mag, phase):\n",
    "        stft_matrix = mag * np.exp(1j*phase)\n",
    "        return librosa.istft(stft_matrix,\n",
    "                             hop_length=self.hp.audio.hop_length,\n",
    "                             win_length=self.hp.audio.win_length)\n",
    "\n",
    "    def amp_to_db(self, x):\n",
    "        return 20.0 * np.log10(np.maximum(1e-5, x))\n",
    "\n",
    "    def db_to_amp(self, x):\n",
    "        return np.power(10.0, x * 0.05)\n",
    "\n",
    "    def normalize(self, S):\n",
    "        return np.clip(S / -self.hp.audio.min_level_db, -1.0, 0.0) + 1.0\n",
    "\n",
    "    def denormalize(self, S):\n",
    "        return (np.clip(S, 0.0, 1.0) - 1.0) * -self.hp.audio.min_level_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u71ZaqvLAciL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/SpeakerDiarization')\n",
    "\n",
    "import configure as c\n",
    "from DB_wav_reader import read_feats_structure\n",
    "from SR_Dataset import read_MFB, ToTensorTestInput\n",
    "#from model_2.model import background_resnet\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4P5dYH_VuYbC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import model_2.resnet as resnet\n",
    "\n",
    "\n",
    "class background_resnet(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, backbone='resnet18'):\n",
    "        super(background_resnet, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # copying modules from pretrained models\n",
    "        if backbone == 'resnet50':\n",
    "            self.pretrained = resnet.resnet50(pretrained=False)\n",
    "        elif backbone == 'resnet101':\n",
    "            self.pretrained = resnet.resnet101(pretrained=False)\n",
    "        elif backbone == 'resnet152':\n",
    "            self.pretrained = resnet.resnet152(pretrained=False)\n",
    "        elif backbone == 'resnet18':\n",
    "            self.pretrained = resnet.resnet18(pretrained=False)\n",
    "        elif backbone == 'resnet34':\n",
    "            self.pretrained = resnet.resnet34(pretrained=False)\n",
    "        else:\n",
    "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
    "            \n",
    "        self.fc0 = nn.Linear(128, embedding_size)\n",
    "        self.bn0 = nn.BatchNorm1d(embedding_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: minibatch x 1 x 40 x 40\n",
    "        x = self.pretrained.conv1(x)\n",
    "        x = self.pretrained.bn1(x)\n",
    "        x = self.pretrained.relu(x)\n",
    "        \n",
    "        x = self.pretrained.layer1(x)\n",
    "        x = self.pretrained.layer2(x)\n",
    "        x = self.pretrained.layer3(x)\n",
    "        x = self.pretrained.layer4(x)\n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(x,1) # [batch, 128, 1, 1]\n",
    "        out = torch.squeeze(out) # [batch, n_embed]\n",
    "        # flatten the out so that the fully connected layer can be connected from here\n",
    "        out = out.view(x.size(0), -1) # (n_batch, n_embed)\n",
    "        spk_embedding = self.fc0(out)\n",
    "        out = F.relu(self.bn0(spk_embedding)) # [batch, n_embed]\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return spk_embedding, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7296,
     "status": "ok",
     "timestamp": 1591169888827,
     "user": {
      "displayName": "Sujin Lee",
      "photoUrl": "",
      "userId": "12954063167699440993"
     },
     "user_tz": -540
    },
    "id": "EAyFMJzqEcDP",
    "outputId": "9378a661-c865-4b35-c5f4-ac20286f691a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.24.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "!pip3 install pydub\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KU7S9VDvE4kf"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.io.wavfile\n",
    "from scipy.fftpack import dct\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzeJvkQgGMnb"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e6BBE1AE80Y"
   },
   "source": [
    "enroll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJdeRD18EJEk"
   },
   "outputs": [],
   "source": [
    "def load_model(use_cuda, log_dir, cp_num, embedding_size, n_classes):\n",
    "    model = background_resnet(embedding_size=embedding_size, num_classes=n_classes)\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    print('=> loading checkpoint')\n",
    "    # original saved file with DataParallel\n",
    "    checkpoint = torch.load(log_dir + '/checkpoint_' + str(cp_num) + '.pth')\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2nWkNOMEPVu"
   },
   "outputs": [],
   "source": [
    "def split_enroll_and_test(dataroot_dir):\n",
    "    DB_all = read_feats_structure(dataroot_dir)\n",
    "    enroll_DB = pd.DataFrame()\n",
    "    test_DB = pd.DataFrame()\n",
    "    \n",
    "    enroll_DB = DB_all[DB_all['filename'].str.contains('enroll.p')]\n",
    "    test_DB = DB_all[DB_all['filename'].str.contains('test.p')]\n",
    "    \n",
    "    # Reset the index\n",
    "    enroll_DB = enroll_DB.reset_index(drop=True)\n",
    "    test_DB = test_DB.reset_index(drop=True)\n",
    "    return enroll_DB, test_DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNSZYob6ESbR"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(use_cuda, filename, model, test_frames):\n",
    "    total = []\n",
    "    input, label = read_MFB(filename) # input size:(n_frames, n_dims)\n",
    "\n",
    "    print(\"len(input) : \", len(input))\n",
    "    \n",
    "    tot_segments = math.ceil(len(input)/test_frames)# total number of segments with 'test_frames' \n",
    "    print(\"tot_segments : \", tot_segments)\n",
    "    \n",
    "    activation = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(tot_segments):\n",
    "            temp_input = input[i*test_frames:i*test_frames+test_frames]\n",
    "            \n",
    "            TT = ToTensorTestInput()\n",
    "            temp_input = TT(temp_input) # size:(1, 1, n_dims, n_frames)\n",
    "    \n",
    "            if use_cuda:\n",
    "                temp_input = temp_input.cuda()\n",
    "            temp_activation,_ = model(temp_input)\n",
    "            total.append(list(np.array(temp_activation)[0]))\n",
    "            activation += torch.sum(temp_activation, dim=0, keepdim=True)\n",
    "    \n",
    "    activation = l2_norm(activation, 1)\n",
    "                \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_oxp5ZhEVob"
   },
   "outputs": [],
   "source": [
    "def l2_norm(input, alpha):\n",
    "    input_size = input.size()  # size:(n_frames, dim)\n",
    "    buffer = torch.pow(input, 2)  # 2 denotes a squared operation. size:(n_frames, dim)\n",
    "    normp = torch.sum(buffer, 1).add_(1e-10)  # size:(n_frames)\n",
    "    norm = torch.sqrt(normp)  # size:(n_frames)\n",
    "    _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "    output = _output.view(input_size)\n",
    "    # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "    output = output * alpha\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoU1fCboF7IK"
   },
   "outputs": [],
   "source": [
    "def extract_section(file_path, wav_file_path):\n",
    "    # Settings\n",
    "    use_cuda = False\n",
    "    log_dir = '/content/drive/My Drive/SpeakerDiarization/model_saved'\n",
    "    embedding_size = 128\n",
    "    cp_num = 24 # Which checkpoint to use?\n",
    "    n_classes = 251\n",
    "    test_frames = 100 #1초당 feature\n",
    "    model = load_model(use_cuda, log_dir, cp_num, embedding_size, n_classes)\n",
    "\n",
    "    test_embedding2 = get_embeddings(use_cuda, file_path, model, test_frames)\n",
    "    \n",
    "    X = test_embedding2\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    print(y_kmeans)\n",
    "    '''plt.scatter(np.array(X)[:, 0], np.array(X)[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)'''\n",
    "    section = [0]\n",
    "    for i in range(0, len(y_kmeans)-1):\n",
    "        if y_kmeans[i] != y_kmeans[i+1]:\n",
    "            section.append((i+1)*1000)\n",
    "    #export sount\n",
    "    sound = AudioSegment.from_wav(wav_file_path)\n",
    "    for i in range(1, len(section)):\n",
    "        sound_cut = sound[section[i-1]:section[i]]\n",
    "        print(section[i-1], section[i])\n",
    "        sound_cut.export(\"/content/drive/My Drive/SpeakerDiarization/output/voice\"+str(i-1)+\".wav\", format=\"wav\")\n",
    "    sound_cut = sound[section[len(section)-1]:]\n",
    "    sound_cut.export(\"/content/drive/My Drive/SpeakerDiarization/output/voice\"+str(len(section)-1)+\".wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8hnwszWFJxx"
   },
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlKuDlnFFLCU"
   },
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    sample_rate, signal = scipy.io.wavfile.read(file)  # File assumed to be in the same directory\n",
    "    #signal = signal[0:int(3.5 * sample_rate)]  # Keep the first 3.5 seconds\n",
    "\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    #25ms 단위로 frame\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "\n",
    "    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(numpy.ceil(float(numpy.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = numpy.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = numpy.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "\n",
    "    indices = numpy.tile(numpy.arange(0, frame_length), (num_frames, 1)) + numpy.tile(numpy.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(numpy.int32, copy=False)]\n",
    "\n",
    "    # window\n",
    "    frames *= numpy.hamming(frame_length)\n",
    "    # frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (frame_length - 1))  # Explicit Implementation **\n",
    "\n",
    "    NFFT = 512\n",
    "    mag_frames = numpy.absolute(numpy.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n",
    "    \n",
    "    #filterbank\n",
    "    nfilt = 40\n",
    "    \n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
    "    mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "    bin = numpy.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = numpy.zeros((nfilt, int(numpy.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = numpy.dot(pow_frames, fbank.T)\n",
    "    filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "    filter_banks = 20 * numpy.log10(filter_banks)  # dB\n",
    "    \n",
    "    return filter_banks\n",
    "\n",
    "#https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9zXjK6lFORJ"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub import silence\n",
    "def make_pickle(file_path, file_name):\n",
    "    wav_file_format = \".wav\"\n",
    "    pickle_file_format = \".p\"\n",
    "    sound = AudioSegment.from_file(file_path + file_name +wav_file_format, format=\"wav\")\n",
    "    sound.export(file_path + \"rm_silence\"+file_name+wav_file_format,format = 'wav')\n",
    "  #test용 pickle file 생성\n",
    "    with open(file_path + \"rm_silence\"+ file_name +pickle_file_format,'wb') as f:\n",
    "        pickle.dump({'feat':preprocessing(file_path+file_name +wav_file_format),'label':'0'},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "alk0AMzQE_xm"
   },
   "source": [
    "voicefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM-P1d6diW0k"
   },
   "outputs": [],
   "source": [
    "def removefile(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        for file in os.scandir(filepath):\n",
    "              os.remove(file.path)\n",
    "        return 'remove all'\n",
    "      else:\n",
    "        return 'no found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y90jDnIKIMUJ"
   },
   "outputs": [],
   "source": [
    "def main(args, hp, output_file):\n",
    "    with torch.no_grad():\n",
    "        model = VoiceFilter(hp).cuda()\n",
    "        chkpt_model = torch.load(args.checkpoint_path)['model']\n",
    "        model.load_state_dict(chkpt_model)\n",
    "        model.eval()\n",
    "\n",
    "        embedder = SpeechEmbedder(hp).cuda()\n",
    "        chkpt_embed = torch.load(args.embedder_path)\n",
    "        embedder.load_state_dict(chkpt_embed)\n",
    "        embedder.eval()\n",
    "\n",
    "        audio = Audio(hp)\n",
    "        dvec_wav, _ = librosa.load(args.reference_file, sr=16000)\n",
    "        dvec_mel = audio.get_mel(dvec_wav)\n",
    "        dvec_mel = torch.from_numpy(dvec_mel).float().cuda()\n",
    "        dvec = embedder(dvec_mel)\n",
    "        dvec = dvec.unsqueeze(0)\n",
    "\n",
    "        mixed_wav, _ = librosa.load(args.mixed_file, sr=16000)\n",
    "        mag, phase = audio.wav2spec(mixed_wav)\n",
    "        mag = torch.from_numpy(mag).float().cuda()\n",
    "\n",
    "        mag = mag.unsqueeze(0)\n",
    "        mask = model(mag, dvec)\n",
    "        est_mag = mag * mask\n",
    "\n",
    "        est_mag = est_mag[0].cpu().detach().numpy()\n",
    "        est_wav = audio.spec2wav(est_mag, phase)\n",
    "\n",
    "        os.makedirs(args.out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(args.out_dir, output_file)\n",
    "        librosa.output.write_wav(out_path, est_wav, sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8305,
     "status": "ok",
     "timestamp": 1591169914206,
     "user": {
      "displayName": "Sujin Lee",
      "photoUrl": "",
      "userId": "12954063167699440993"
     },
     "user_tz": -540
    },
    "id": "kKmJOmWpKf_e",
    "outputId": "87eec688-7e88-493d-ba8e-64d8cf8e92bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "len(input) :  1524\n",
      "tot_segments :  16\n",
      "[0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 1]\n",
      "0 7000\n",
      "7000 15000\n",
      "delete\n"
     ]
    }
   ],
   "source": [
    "file_name = \"rand_overlay1\"\n",
    "file_path = \"/content/drive/My Drive/SpeakerDiarization/\"\n",
    "wav_file_path = file_path + file_name\n",
    "pickle_file_path = file_path + \"rm_silence\"+file_name+\".p\"\n",
    "rm_silence_wav_file_path = file_path + \"rm_silence\" + file_name+\".wav\"\n",
    "make_pickle(file_path, file_name)\n",
    "extract_section(pickle_file_path, rm_silence_wav_file_path)\n",
    "if os.path.isdir(\"/content/drive/My Drive/SpeakerDiarization/output/.ipynb_checkpoints\"):\n",
    "    os.rmdir(\"/content/drive/My Drive/SpeakerDiarization/output/.ipynb_checkpoints\")\n",
    "    print(\"delete\")\n",
    "file_list = os.listdir(\"/content/drive/My Drive/SpeakerDiarization/output\")\n",
    "for i in file_list:\n",
    "    try:\n",
    "        args = easydict.EasyDict({\n",
    "            \"config\": \"/content/drive/My Drive/voicefilter-master/config/config.yaml\",\n",
    "            \"embedder_path\": \"/content/drive/My Drive/voicefilter-master/embedder.pt\",\n",
    "            \"checkpoint_path\": \"/content/drive/My Drive/voicefilter-master/chkpt/train/chkpt_70000.pt\",\n",
    "            \"mixed_file\": \"/content/drive/My Drive/SpeakerDiarization/\"+file_name+\".wav\",\n",
    "            \"reference_file\": \"/content/drive/My Drive/SpeakerDiarization/output/\"+i,\n",
    "            \"out_dir\": \"/content/drive/My Drive/voicefilter-master/result\"})\n",
    "        hp = HParam(args.config)\n",
    "        main(args, hp, \"result_\"+i)\n",
    "    except RuntimeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
